{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data items of both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mypath = \".//home//images//\"\n",
    "filename = [f for f in listdir(mypath) if isfile(join(mypath,f))]\n",
    "\n",
    "print(str(len(filename))+' images loaded')\n",
    "\n",
    "def make_dir(directory):\n",
    "        if os.path.exists(directory):\n",
    "            shutil.rmtree(directory)\n",
    "        os.makedirs(directory)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "bio_count = 0\n",
    "non_bio_count = 0\n",
    "size = 150\n",
    "train_image = []\n",
    "train_labels = []\n",
    "test_image = []\n",
    "test_labels = []\n",
    "train_size = 270\n",
    "test_size = 130\n",
    "\n",
    "bio_dir_train = './/home/crowd/train/bio/'\n",
    "non_bio_dir_train = './/home/crowd/train/non_bio/'\n",
    "bio_dir_val = './/home/crowd/valid/bio/'\n",
    "non_bio_dir_val = './/home/crowd/valid/non_bio/'\n",
    "\n",
    "make_dir(bio_dir_train)\n",
    "make_dir(non_bio_dir_train)\n",
    "make_dir(bio_dir_val)\n",
    "make_dir(non_bio_dir_val)\n",
    "\n",
    "\n",
    "for i,file in enumerate(filename):\n",
    "    \n",
    "    if(file[0] == 'b'):\n",
    "        if(bio_count >= train_size+test_size):\n",
    "            continue\n",
    "        else:\n",
    "            img=cv2.imread(mypath+file)\n",
    "            img=cv2.resize(img,(size,size),interpolation=cv2.INTER_AREA)\n",
    "            train_image.append(img)\n",
    "            train_labels.append('0')\n",
    "            cv2.imwrite(bio_dir_train+'bio'+str(i)+'.jpg',img)        \n",
    "                \n",
    "    if(file[0] == 'n'):\n",
    "            img=cv2.imread(mypath+file)\n",
    "            img=cv2.resize(img,(size,size),interpolation=cv2.INTER_AREA)\n",
    "            train_image.append(img)\n",
    "            train_labels.append('1')\n",
    "            cv2.imwrite(non_bio_dir_train+'non_bio'+str(i)+'.jpg',img)\n",
    "\n",
    "print('data loaded sucessfully')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Waste_train_data.npz',np.array(train_image))\n",
    "np.savez('Waste_train_labels.npz',np.array(train_labels))\n",
    "np.savez('Waste_test_data.npz',np.array(test_image))\n",
    "np.savez('Waste_test_labels.npz',np.array(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    npzfile = np.load(dataset+'_train_data.npz')\n",
    "    train = npzfile['arr_0']\n",
    "    \n",
    "    npzfile = np.load(dataset+'_train_labels.npz')\n",
    "    train_labels = npzfile['arr_0']\n",
    "    \n",
    "    npzfile = np.load(dataset+'_test_data.npz')\n",
    "    test = npzfile['arr_0']\n",
    "    \n",
    "    npzfile = np.load(dataset+'_test_labels.npz')\n",
    "    test_labels = npzfile['arr_0']\n",
    "    \n",
    "    return (train,train_labels),(test,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIODEGRADABLE\n",
      "BIODEGRADABLE\n",
      "BIODEGRADABLE\n",
      "BIODEGRADABLE\n",
      "NON BIODEGRADABLE\n",
      "NON BIODEGRADABLE\n",
      "NON BIODEGRADABLE\n",
      "BIODEGRADABLE\n",
      "NON BIODEGRADABLE\n",
      "NON BIODEGRADABLE\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    val=np.random.randint(0,len(train_labels))\n",
    "    #print(train_image[val])\n",
    "    cv2.imshow(\"image_\"+str(i),train_image[val])\n",
    "    \n",
    "    if(train_labels[val]=='0'):\n",
    "        print('BIODEGRADABLE')\n",
    "    else:\n",
    "        print('NON BIODEGRADABLE')\n",
    "    cv2.waitKey(0) \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = load_data('Waste')\n",
    "\n",
    "y_train=y_train.reshape(y_train.shape[0],1)\n",
    "y_test=y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "x_train=x_train.astype('float32')\n",
    "x_test=x_test.astype('float32')\n",
    "\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Keras Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model based on Convolution Neural Network algorithum of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture consist of :\n",
    "\n",
    "1.) A convolution layer   \n",
    "2.) Followed by Relu Activation Layer   \n",
    "3.) Followed by MaxPooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "# Number of Data points in one epoch\n",
    "batch_size = 16\n",
    "\n",
    "# Total iterative epochs \n",
    "epochs = 15\n",
    "\n",
    "# Organising the input Data Shape\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[1].shape[0]\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Model compilation with loss as \"binary crossentropy\" and optimizer as \"rmsprop\"\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Visualizing our model Architecture \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Training of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 519 samples, validate on 3 samples\n",
      "Epoch 1/15\n",
      "519/519 [==============================] - 21s 40ms/step - loss: 0.7264 - acc: 0.6050 - val_loss: 0.2753 - val_acc: 1.0000\n",
      "Epoch 2/15\n",
      "519/519 [==============================] - 20s 38ms/step - loss: 0.5312 - acc: 0.7418 - val_loss: 0.1233 - val_acc: 1.0000\n",
      "Epoch 3/15\n",
      "519/519 [==============================] - 20s 38ms/step - loss: 0.5108 - acc: 0.7514 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "Epoch 4/15\n",
      "519/519 [==============================] - 19s 36ms/step - loss: 0.4783 - acc: 0.8035 - val_loss: 0.1889 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      "519/519 [==============================] - 20s 38ms/step - loss: 0.4550 - acc: 0.8150 - val_loss: 0.3679 - val_acc: 0.6667\n",
      "Epoch 6/15\n",
      "519/519 [==============================] - 21s 40ms/step - loss: 0.4802 - acc: 0.8478 - val_loss: 0.1536 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      "519/519 [==============================] - 21s 40ms/step - loss: 0.3289 - acc: 0.8748 - val_loss: 0.0537 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "519/519 [==============================] - 20s 39ms/step - loss: 0.3323 - acc: 0.8825 - val_loss: 0.1259 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      "519/519 [==============================] - 24s 45ms/step - loss: 0.3213 - acc: 0.8960 - val_loss: 0.0239 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "519/519 [==============================] - 20s 39ms/step - loss: 0.2448 - acc: 0.9075 - val_loss: 0.1838 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "519/519 [==============================] - 20s 39ms/step - loss: 0.2397 - acc: 0.9287 - val_loss: 0.0474 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "519/519 [==============================] - 20s 39ms/step - loss: 0.2146 - acc: 0.9229 - val_loss: 0.1657 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "519/519 [==============================] - 21s 40ms/step - loss: 0.2244 - acc: 0.9345 - val_loss: 0.7824 - val_acc: 0.6667\n",
      "Epoch 14/15\n",
      "519/519 [==============================] - 20s 38ms/step - loss: 0.1128 - acc: 0.9672 - val_loss: 2.0890 - val_acc: 0.6667\n",
      "Epoch 15/15\n",
      "519/519 [==============================] - 19s 37ms/step - loss: 0.1646 - acc: 0.9403 - val_loss: 0.0693 - val_acc: 1.0000\n",
      "3/3 [==============================] - 0s 14ms/step\n",
      "Test loss: 0.06925424933433533\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the performance of our trained model\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model api for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Waste_95.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing on validation Data and cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "classifier = load_model('Waste_95.h5')\n",
    "\n",
    "def draw_test(name, pred, input_im):\n",
    "    BLACK = [0,0,0]\n",
    "    if pred == \"[[0]]\":\n",
    "        pred = \"BIO\"\n",
    "    if pred == \"[[1]]\":\n",
    "        pred = \"NON BIO\"\n",
    "    expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0] ,\n",
    "                                        cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    #expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.putText(expanded_image, str(pred), (252, 70) , \n",
    "                cv2.FONT_HERSHEY_COMPLEX_SMALL,4, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    val=np.random.randint(0,len(y_test))\n",
    "    input_im=x_test[val]\n",
    "    \n",
    "    imageL=cv2.resize(input_im,None,fx=2,fy=2,interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    cv2.imshow('image',imageL)\n",
    "    \n",
    "    input_im = input_im.reshape(1,150,150,3) \n",
    "    \n",
    "    ## Get Prediction\n",
    "    res = str(classifier.predict_classes(input_im, 1, verbose = 0))\n",
    "    \n",
    "    draw_test(\"Prediction\", res, imageL) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tesing on Live frame on Real-Time Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "classifier=load_model('Waste_95.h5')\n",
    "def draw_text(name, pred, input_im):\n",
    "    BLACK = [0,0,0]\n",
    "    #expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0] ,\n",
    "                                       # cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    #expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.putText(input_im, pred, (3, 70) , \n",
    "                cv2.FONT_HERSHEY_COMPLEX_SMALL,2, (255,0,0), 3)\n",
    "    cv2.imshow(name, input_im)\n",
    "\n",
    "cam=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    yes,frame=cam.read()\n",
    "\n",
    "    frame1=frame\n",
    "    frame=cv2.resize(frame,(150,150),interpolation=cv2.INTER_AREA)\n",
    "    frame=frame.astype('float32')\n",
    "    \n",
    "    frame=frame/255\n",
    "    \n",
    "    frame=frame.reshape(1,150,150,3)\n",
    "    res=str(classifier.predict_classes(frame,1,verbose=0))\n",
    "    if(res=='[[0]]'):\n",
    "        draw_text('final','BIODEGRADABLE',frame1)\n",
    "    \n",
    "    if(res=='[[1]]'):\n",
    "        draw_text('final','NON_BIODEGRADABLE',frame1)\n",
    "    if cv2.waitKey(1)==27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
